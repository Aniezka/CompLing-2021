{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19a113d",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c7065",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fae940e",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983165b6",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ee1ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0dc5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "dvach = open('2ch_corpus.txt', encoding = 'utf8').read()\n",
    "news = open('lenta.txt', encoding = 'utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1edd84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина 1 - 11638405\n",
      "Длина 2 - 11536552\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина 1 -\", len(dvach))\n",
    "print(\"Длина 2 -\", len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "febe492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46e1a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_news = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)][:-50]\n",
    "sentence_test = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "246ab4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "280dbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence))\n",
    "    trigrams_news.update(ngrammer(sentence, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19d4f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy \n",
    "\n",
    "matrix_news = scipy.sparse.lil_matrix((len(bigrams_news), len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "id2bigram_news = list(bigrams_news)\n",
    "bigram2id_news = {word:i for i, word in enumerate(id2bigram_news)}\n",
    "\n",
    "# вероятность расчитываем точно также\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + \" \" + word2\n",
    "    trigram = word1 + \" \" + word2 + \" \" + word3\n",
    "    \n",
    "    matrix_news[bigram2id_news[bigram], word2id_news[word3]] =  (trigrams_news[trigram] / bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "472f82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(matrix, id2word, word2id, id2bigram, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    prev = start\n",
    "    for i in range(n):\n",
    "        prev = prev.split()[1]\n",
    "\n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen])\n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "            prev = \"<start> <start>\"\n",
    "        else:\n",
    "            prev = prev + \" \" + id2word[chosen]\n",
    "        \n",
    "        current_idx = bigram2id[prev]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1551da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тип и происхождение мины установлены это минометная мина с оперением времен великой отечественной войны \n",
      " кеннеди на мысе канаверал осталось всего 102 добровольца остальные эвакуированы \n",
      " в настоящее время анализируется \n",
      " генерал особо подчеркнул что решение направить делегацию fbi в российскуюстолицу это острая реакция правительства сша на происходящиесобытия в москве с 6 сентября поисковые работы \n",
      " арестованы 9 человек которые подозреваются в получении взяток и злоупотреблении общественными фондами во время налета на югославию \n",
      " кроме прейскера нашими войсками взят в плен \n",
      " высокая активность добровольцев позволила специалистам в короткий срок создать минимальный неприкосновенный резерв донорской крови и ее гражданам полной\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news, id2bigram_news, bigram2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7044b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    \n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549ce29",
   "metadata": {},
   "source": [
    "Перплексия, усреднённая по 10-ти отложенным предложениям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54f0e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = []\n",
    "for sentence in sentence_test[:10]:\n",
    "    prob = []\n",
    "    \n",
    "    for ngram in ngrammer(sentence, n=3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "\n",
    "        gram2 = word1 + ' ' + word2\n",
    "        if gram2 in bigrams_news and ngram in trigrams_news:\n",
    "            prob.append(np.log(trigrams_news[ngram] / bigrams_news[gram2]))\n",
    "        else:\n",
    "            prob.append(np.log(0.00001))\n",
    "    \n",
    "    perplexities.append(perplexity(prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20a2b995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34172.911477570065,\n",
       " 22587.82763143729,\n",
       " 14046.609087247294,\n",
       " 99999.9999999999,\n",
       " 62630.64674967313,\n",
       " 99999.9999999999,\n",
       " 84530.6512373702,\n",
       " 65691.42386718343,\n",
       " 99999.99999999991,\n",
       " 20890.3675130843]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c2a61dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mean perplexity: 60455.04375635654'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'mean perplexity: {np.mean(perplexities)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a413378d",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473febc0",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6d6393",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b808d",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf0bc0",
   "metadata": {},
   "source": [
    "Несловарные (OOV) слова – это слова, которых нет в обучающих данных, но они присутствуют в тестовом наборе. В системе с открытым словарем (open vocabulary) мы моделируем эти потенциально OOV-слова на тестовом наборе, добавляя псевдослово $<UNK>$. \n",
    "В главе утверждается, что существует два способа работы с $<UNK>$:\n",
    "1.\tВыбрать заранее фиксированный словарь, преобразовать любое OOV-слово в токен неизвестного слова $<UNK>$ на трейне на этапе нормализации текста, вычислить вероятности для $<UNK>$ по его встречаемости, как и для любого другого обычного слова на трейне.\n",
    "2.\tЕсли нет заранее составленного словаря нужно его неявно создать, заменяя слова в обучающих данных на $<UNK>$ на основе их частоты. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ed4c0",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d537d74",
   "metadata": {},
   "source": [
    "Бывает так, что слово есть в словаре, но оно появляется в незнакомом контексте в тестовом наборе. Сглаживание (smoothing или discounting) - процесс, когда таким событиям присваивается не нулевая вероятность, а какая-то часть вероятности, которая берётся у более частотных событий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7dbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
