{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8463f824",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e9f04",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b74702",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "421beaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5d3720ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
    "    # 1 вектора ко всей матрице (словам в словаре)\n",
    "    # считать по отдельности циклом было бы дольше\n",
    "    # вместо одного вектора может даже целая матрица\n",
    "    # тогда считаться в итоге будет ещё быстрее\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]\n",
    "\n",
    "\n",
    "def get_closest_match_with_metric(text, lookup, topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "    \n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=10, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "    closest_with_freq = [(word, dist, P(text, N)) for word, dist in closest]\n",
    "    # сначала сортируем по расстоянию, потом по частоте встречаемости\n",
    "    return sorted(closest_with_freq, key=lambda x: (x[1], x[2]), reverse=True)\n",
    "\n",
    "\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3fd4afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "corpus = open('wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9ff1bf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=1000)\n",
    "X = vec.fit_transform(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c5a984ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('солнечный', 1.0, 4.8493980636159555e-06),\n",
       " ('солнечные', 0.8888888888888888, 4.8493980636159555e-06),\n",
       " ('солнечным', 0.8888888888888888, 4.8493980636159555e-06),\n",
       " ('солнечных', 0.8888888888888888, 4.8493980636159555e-06),\n",
       " ('солнечной', 0.8888888888888888, 4.8493980636159555e-06),\n",
       " ('солнечными', 0.8, 4.8493980636159555e-06),\n",
       " ('конечный', 0.7777777777777778, 4.8493980636159555e-06),\n",
       " ('солнечно', 0.7777777777777778, 4.8493980636159555e-06),\n",
       " ('солнечное', 0.7777777777777778, 4.8493980636159555e-06),\n",
       " ('соленый', 0.7777777777777778, 4.8493980636159555e-06)]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_hybrid_match('солнечный', X, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bd33473e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('солнце', 1.0, 2.4440966240624417e-05),\n",
       " ('солнцем', 0.8571428571428572, 2.4440966240624417e-05),\n",
       " ('солнцев', 0.8571428571428572, 2.4440966240624417e-05),\n",
       " ('солнцу', 0.8333333333333334, 2.4440966240624417e-05),\n",
       " ('солнца', 0.8333333333333334, 2.4440966240624417e-05),\n",
       " ('солнцеву', 0.75, 2.4440966240624417e-05),\n",
       " ('солнцева', 0.75, 2.4440966240624417e-05),\n",
       " ('солонцы', 0.7142857142857143, 2.4440966240624417e-05),\n",
       " ('соляное', 0.7142857142857143, 2.4440966240624417e-05),\n",
       " ('солнцевой', 0.6666666666666667, 2.4440966240624417e-05)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_hybrid_match('солнце', X, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad7c11",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dd4bf",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ee3ecc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_word = list(vocab.keys())\n",
    "deleted_char = {}\n",
    "for word in correct_word:\n",
    "    for i in range(1, len(word) + 1):\n",
    "        tmp = word[:i-1] + word[i:]\n",
    "        if tmp in deleted_char.keys():\n",
    "            deleted_char[tmp].append(word)\n",
    "        else:  \n",
    "            deleted_char[tmp] = [word]\n",
    "#     deleted_char.update(temp_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "db617a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('овостройка', ['новостройка']),\n",
       " ('нвостройка', ['новостройка']),\n",
       " ('ноостройка', ['новостройка']),\n",
       " ('новстройка', ['новостройка']),\n",
       " ('новотройка', ['новостройка']),\n",
       " ('новосройка', ['новостройка']),\n",
       " ('новостойка', ['новостройка']),\n",
       " ('новострйка', ['новостройка']),\n",
       " ('новострока', ['новостройка']),\n",
       " ('новостройа', ['новостройка']),\n",
       " ('новостройк', ['новостройка', 'новостройки']),\n",
       " ('ижегородская', ['нижегородская']),\n",
       " ('нжегородская', ['нижегородская']),\n",
       " ('ниегородская', ['нижегородская']),\n",
       " ('нижгородская', ['нижегородская'])]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(deleted_char.items())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "62ddcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_error(text, deleted_char):\n",
    "    similar = set()\n",
    "    for i in range(1, len(text) + 1):\n",
    "        if text[:i - 1] + text[i:] in deleted_char.keys():\n",
    "            for word in deleted_char[text[:i - 1] + text[i:]]:\n",
    "                similar.add(word)\n",
    "    similar_freq = [(word, P(word, N)) for word in similar]\n",
    "    if len(similar_freq) == 0:\n",
    "        return \" \"\n",
    "    return sorted(similar_freq, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b3b72e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('солнца', 3.220000314240995e-05),\n",
       " ('солнце', 2.4440966240624417e-05),\n",
       " ('солнцу', 3.1036147607142114e-06),\n",
       " ('олонце', 3.879518450892764e-07)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_error('солнце', deleted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f968edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))\n",
    "\n",
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], fix_error(pair[1], deleted_char)[0][0])\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "    \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f145dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8185092546273136\n",
      "0.20031055900621117\n",
      "0.09004249454461927\n"
     ]
    }
   ],
   "source": [
    "print(correct / total)\n",
    "print(mistaken_fixed / total_mistaken)\n",
    "print(correct_broken / total_correct) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3650687",
   "metadata": {},
   "source": [
    "## *3. Настройка гиперпараметров. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84680a",
   "metadata": {},
   "source": [
    "У метода из первого заданий много гиперпараметров (те которые нужно подбирать самостоятельно). Это параметры векторайзера, topn, метрика редактирования. Поэкспериментируйте с ними. \n",
    "\n",
    "Проведите как минимум 10 экспериментов с разными параметрами. Для каждого эксперимента укажите мотивацию (например, \"слишком маленький topn в get_closest_match_vec приводит к тому, что некоторые хорошие варианты не доходят до get_closest_match_with_metric, попробуем его увеличить\")\n",
    "\n",
    "Старайтесь получить улучшение, но если улучшить не получится, то это не страшно. Главное, чтобы эксперименты были осмысленными, а не рандомными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e3bd076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(func, *args, **kwargs):\n",
    "    mistakes = []\n",
    "    total_mistaken = 0\n",
    "    mistaken_fixed = 0\n",
    "\n",
    "    total_correct = 0\n",
    "    correct_broken = 0\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    cashed = {}\n",
    "    for i in range(len(true)):\n",
    "        word_pairs = align_words(true[i], bad[i])\n",
    "        for pair in word_pairs:\n",
    "            if predict_mistaken(pair[1], vocab):\n",
    "                pred = cashed.get(pair[1], func(pair[1], X, vec, *args, **kwargs)[0][0])\n",
    "                cashed[pair[1]] = pred\n",
    "            else:\n",
    "                pred = pair[1]\n",
    "\n",
    "\n",
    "            if pred == pair[0]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                mistakes.append((pair[0], pair[1], pred))\n",
    "            total += 1\n",
    "\n",
    "            if pair[0] == pair[1]:\n",
    "                total_correct += 1\n",
    "                if pair[0] != pred:\n",
    "                    correct_broken += 1\n",
    "            else:\n",
    "                total_mistaken += 1\n",
    "                if pair[0] == pred:\n",
    "                    mistaken_fixed += 1\n",
    "\n",
    "        if not i % 100:\n",
    "            print(i)\n",
    "    return correct / total, mistaken_fixed / total_mistaken, correct_broken / total_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fff26d",
   "metadata": {},
   "source": [
    "### Посчитаем качество стандартного алгоритма, на дефолтных параметрах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b908982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=1000)\n",
    "X = vec.fit_transform(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "04d48acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.854\n",
      "mistaken_fixed/total_mistaken: 0.473\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "res = get_metrics(get_closest_hybrid_match)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c85570",
   "metadata": {},
   "source": [
    "### 1) Слишком маленький topn в get_closest_match_vec приводит к тому, что некоторые хорошие варианты не доходят до get_closest_match_with_metric, попробуем его увеличить до 30-ти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0e40cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.854\n",
      "mistaken_fixed/total_mistaken: 0.474\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "res = get_metrics(get_closest_hybrid_match, topn=30)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a9955f",
   "metadata": {},
   "source": [
    "Изменения незначительные (mistaken_fixed/total_mistaken: 0.473 -> 0.474), попробуем увеличить topn еще сильнее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cfad4",
   "metadata": {},
   "source": [
    "### 2) Слишком маленький topn в get_closest_match_vec приводит к тому, что некоторые хорошие варианты не доходят до get_closest_match_with_metric, попробуем его увеличить до 50-ти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3d503243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.854\n",
      "mistaken_fixed/total_mistaken: 0.475\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "res = get_metrics(get_closest_hybrid_match, topn=50)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c8eb2",
   "metadata": {},
   "source": [
    "Изменения опять незначительные, но чуть лучше Эксперимента 1; mistaken_fixed/total_mistaken: 0.473 -> 0.475 по сравнению со стандартными значениями параметров)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3f9e1",
   "metadata": {},
   "source": [
    "### 3) Может метод подсчёта расстояний влияет на правильность интерпретации разницы между словами, рассмотрим textdistance.hamming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f9953c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.826\n",
      "mistaken_fixed/total_mistaken: 0.258\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "res = get_metrics(get_closest_hybrid_match, metric=textdistance.hamming)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a9bf63",
   "metadata": {},
   "source": [
    "Результат изменения подсчета функции расстояния ощутимо хуже результатов стандартной функции Левенштейна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4621a",
   "metadata": {},
   "source": [
    "### 4) Может метод подсчёта расстояний влияет на правильность интерпретации разницы между словами, рассмотрим textdistance.cosine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c65a2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.841\n",
      "mistaken_fixed/total_mistaken: 0.375\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "res = get_metrics(get_closest_hybrid_match, metric=textdistance.cosine)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d44f0",
   "metadata": {},
   "source": [
    "Функция косинусного расстояния показала результаты лучше, чем функция Хэмминга, но все равно хуже, чем стандартная функция"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e32bd8",
   "metadata": {},
   "source": [
    "### 5) Может метод подсчёта расстояний влияет на правильность интерпретации разницы между словами, рассмотрим textdistance.strcmp95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "584549c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.849\n",
      "mistaken_fixed/total_mistaken: 0.439\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "res = get_metrics(get_closest_hybrid_match, metric=textdistance.strcmp95)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c4ed5",
   "metadata": {},
   "source": [
    "Функция подсчета расстояния между строками показала хороший результат, но все равно хуже, чем стандратный результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90358d24",
   "metadata": {},
   "source": [
    "### 6) Попробуем увеличить max_features при векторизации, таким образом сохранив больше информации о словах (1000 -> 2000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "530bf539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.854\n",
      "mistaken_fixed/total_mistaken: 0.477\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=2000)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "res = get_metrics(get_closest_hybrid_match)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b644ece9",
   "metadata": {},
   "source": [
    "Результаты ощутимо лучше, попробуем увеличить max_features еще сильнее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad2979",
   "metadata": {},
   "source": [
    "### 7) Попробуем увеличить max_features при векторизации, таким образом сохранив больше информации о словах (1000 -> 4000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9e506977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.855\n",
      "mistaken_fixed/total_mistaken: 0.482\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=4000)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "res = get_metrics(get_closest_hybrid_match)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa064825",
   "metadata": {},
   "source": [
    "Наверное, увеличение max_features еще сильнее улучшит результат, но это считается слишком долго ("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d6975e",
   "metadata": {},
   "source": [
    "### 8) Попробуем увеличить ngram_range при векторизации, это поможет рассмотреть больше различных комбинаций букв ( (1, 3) -> (1, 4) ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8303cc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.853\n",
      "mistaken_fixed/total_mistaken: 0.471\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=1000)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "res = get_metrics(get_closest_hybrid_match)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb388e7",
   "metadata": {},
   "source": [
    "Увеличение количества рассматриваемых токенов путем увеличения их длины не улучшило результат"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8319d",
   "metadata": {},
   "source": [
    "### 9) С помощью комбинации увеличения ngram_range и max_features при векторизации можно добится что чуть более редкие n-граммы всё равно будут рассматриватся в дальнейшем при векторизации ( (1, 3) -> (1, 4); 1000 -> 2000 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "68368e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.854\n",
      "mistaken_fixed/total_mistaken: 0.475\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,4), max_features=2000)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "res = get_metrics(get_closest_hybrid_match)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f885f05",
   "metadata": {},
   "source": [
    "Комбинация увеличения длины ngram и max_features улучшила результат, но это связано с тем, что само по себе увеличение max_features показывает еще бОльший результат, поэтому увеличение длины ngram не улучшает качество "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4b1c6",
   "metadata": {},
   "source": [
    "### 10) С помощью комбинации увеличения topn при отборе и max_features при векторизации можно добится что больше слов будут рассматриваться после перемножения увеличенных по размерности векторов ( 20 -> 50; 1000 -> 2000 ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c44b3fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "correct/total: 0.854\n",
      "mistaken_fixed/total_mistaken: 0.474\n",
      "correct_broken/total_correct: 0.09\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=2000)\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "res = get_metrics(get_closest_hybrid_match, topn=50)\n",
    "\n",
    "print(f\"correct/total: {round(res[0], 3)}\")\n",
    "print(f\"mistaken_fixed/total_mistaken: {round(res[1], 3)}\")\n",
    "print(f\"correct_broken/total_correct: {round(res[2], 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0041d6f",
   "metadata": {},
   "source": [
    "Комбинация увеличения topn и max_features не дает прироста в качестве"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa53890",
   "metadata": {},
   "source": [
    "### Лучший результат – способ увеличения max_features (рассматривается больше различных ngram, увеличивается размерность векторов). Остальные эксперименты не привели к желаемому улучшению."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963ddad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
