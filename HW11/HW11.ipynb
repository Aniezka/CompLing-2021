{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTZAsMb8l5e8"
   },
   "source": [
    "\n",
    "# Домашнее задание № 11. Машинный перевод\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX5lZRwYl_Lb"
   },
   "source": [
    "## Задание 1 (6 баллов + 2 доп балла).\n",
    "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
    "\n",
    "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQA-i-w2hN50",
    "outputId": "22b8a5a8-b6d5-4a9b-871d-91d02a704b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers matplotlib sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "qjr54hCcmdcI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tD17nQaok4J",
    "outputId": "4a0df345-485b-4bd3-efb8-01081864da4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-22 18:25:45--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 121340806 (116M)\n",
      "Saving to: ‘opus.en-ru-train.ru.2’\n",
      "\n",
      "opus.en-ru-train.ru 100%[===================>] 115.72M  95.8MB/s    in 1.2s    \n",
      "\n",
      "2022-05-22 18:25:46 (95.8 MB/s) - ‘opus.en-ru-train.ru.2’ saved [121340806/121340806]\n",
      "\n",
      "--2022-05-22 18:25:46--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 67760131 (65M)\n",
      "Saving to: ‘opus.en-ru-train.en.2’\n",
      "\n",
      "opus.en-ru-train.en 100%[===================>]  64.62M  89.1MB/s    in 0.7s    \n",
      "\n",
      "2022-05-22 18:25:47 (89.1 MB/s) - ‘opus.en-ru-train.en.2’ saved [67760131/67760131]\n",
      "\n",
      "--2022-05-22 18:25:47--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 305669 (299K)\n",
      "Saving to: ‘opus.en-ru-test.ru.2’\n",
      "\n",
      "opus.en-ru-test.ru. 100%[===================>] 298.50K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-05-22 18:25:47 (3.32 MB/s) - ‘opus.en-ru-test.ru.2’ saved [305669/305669]\n",
      "\n",
      "--2022-05-22 18:25:48--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
      "Resolving data.statmt.org (data.statmt.org)... 129.215.197.184\n",
      "Connecting to data.statmt.org (data.statmt.org)|129.215.197.184|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 173307 (169K)\n",
      "Saving to: ‘opus.en-ru-test.en.2’\n",
      "\n",
      "opus.en-ru-test.en. 100%[===================>] 169.25K  --.-KB/s    in 0.07s   \n",
      "\n",
      "2022-05-22 18:25:48 (2.44 MB/s) - ‘opus.en-ru-test.en.2’ saved [173307/173307]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
    "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "KxetGU3xphfR"
   },
   "outputs": [],
   "source": [
    "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
    "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "dUj35DhLpo2e"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = Tokenizer(WordPiece(), )\n",
    "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_en = WordPieceTrainer(vocab_size=30000,\n",
    "                              special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "tokenizer_en.train(files=[\"opus.en-ru-train.en\"],\n",
    "                   trainer=trainer_en )\n",
    "\n",
    "tokenizer_ru = Tokenizer(WordPiece(), )\n",
    "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
    "tokenizer_ru.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer_ru = WordPieceTrainer(vocab_size=30000,\n",
    "                              special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"])\n",
    "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"],\n",
    "                   trainer=trainer_ru )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "MLHMWiHdpy9M"
   },
   "outputs": [],
   "source": [
    "tokenizer_en.decoder = decoders.WordPiece()\n",
    "tokenizer_ru.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "hsT5dxCYp7ih"
   },
   "outputs": [],
   "source": [
    "def encode(text, tokenizer, target=False):\n",
    "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "pGeqxLnnp7cx"
   },
   "outputs": [],
   "source": [
    "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
    "X_ru = [encode(t, tokenizer_ru, target=True) for t in ru_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "iblpWrHHqKjS"
   },
   "outputs": [],
   "source": [
    "max_len_en, max_len_ru = 20, 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "jFKHOhUusv0V"
   },
   "outputs": [],
   "source": [
    "PAD_IDX = tokenizer_en.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "XSzz2LSXp7YO"
   },
   "outputs": [],
   "source": [
    "X_en = tf.keras.preprocessing.sequence.pad_sequences(X_en,\n",
    "                                                     maxlen=max_len_en,\n",
    "                                                     padding='post',\n",
    "                                                     value=tokenizer_en.token_to_id('[PAD]'))\n",
    "\n",
    "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences([x[1:] for x in X_ru],\n",
    "                                                         maxlen=max_len_ru-1, padding='post', \n",
    "                                                         value=tokenizer_en.token_to_id('[PAD]'))\n",
    "\n",
    "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences([x[:-1] for x in X_ru],\n",
    "                                                         maxlen=max_len_ru-1,\n",
    "                                                         padding='post', value=tokenizer_ru.token_to_id('[PAD]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "iGd8zSAep7TI"
   },
   "outputs": [],
   "source": [
    "X_en_train, X_en_valid, X_ru_dec_train, X_ru_dec_valid, X_ru_out_train, X_ru_out_valid = train_test_split(X_en, \n",
    "                                                                                                          X_ru_dec, \n",
    "                                                                                                          X_ru_out, \n",
    "                                                                                                          test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "HxKS7eNlp7O8"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "UWpatIERp7Kx"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "HOfbjw8rqtcp"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "v2Woi1KZqtZn"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "Sqi3x97mqtXC"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "oHPacafEqtUG"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "9Og4CeQxqtPu"
   },
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "V2pgfzoHp7Da"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "O3bGWINop63y"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            max_len,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "beKbbHAYq2pt"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                max_len,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size[0],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[0],\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size[1],\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      max_len=max_len[1],\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "zemAdJEyq2lg"
   },
   "outputs": [],
   "source": [
    "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none',)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    loss = L(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "pz3medPxq2ja"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Egu2pif_q2gE",
    "outputId": "ce093cf8-502b-49e7-de07-d2776372c135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# small model\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    model = transformer(\n",
    "        vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        max_len=[max_len_en, max_len_ru])\n",
    "\n",
    "#     learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def accuracy(y_true, y_pred):\n",
    "#         y_true = tf.reshape(y_true, shape=(-1, max_len_ru - 1))\n",
    "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1,\n",
    "                                            save_weights_only=True,\n",
    "                                            save_best_only=True,\n",
    "                                            mode='min',\n",
    "                                            save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkYjnVRlq2TO",
    "outputId": "de4aea92-a704-4a53-b089-0871ed5a2ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1856/1856 [==============================] - ETA: 0s - loss: 2.4584 - accuracy: 0.2022\n",
      "Epoch 1: val_loss improved from inf to 1.79796, saving model to model_ruen\n",
      "1856/1856 [==============================] - 719s 383ms/step - loss: 2.4584 - accuracy: 0.2022 - val_loss: 1.7980 - val_accuracy: 0.2634\n",
      "Epoch 2/6\n",
      "1856/1856 [==============================] - ETA: 0s - loss: 1.6788 - accuracy: 0.2736\n",
      "Epoch 2: val_loss improved from 1.79796 to 1.57780, saving model to model_ruen\n",
      "1856/1856 [==============================] - 708s 381ms/step - loss: 1.6788 - accuracy: 0.2736 - val_loss: 1.5778 - val_accuracy: 0.2876\n",
      "Epoch 3/6\n",
      "1856/1856 [==============================] - ETA: 0s - loss: 1.4996 - accuracy: 0.2933\n",
      "Epoch 3: val_loss improved from 1.57780 to 1.49602, saving model to model_ruen\n",
      "1856/1856 [==============================] - 708s 382ms/step - loss: 1.4996 - accuracy: 0.2933 - val_loss: 1.4960 - val_accuracy: 0.2984\n",
      "Epoch 4/6\n",
      "1856/1856 [==============================] - ETA: 0s - loss: 1.4059 - accuracy: 0.3042\n",
      "Epoch 4: val_loss improved from 1.49602 to 1.45368, saving model to model_ruen\n",
      "1856/1856 [==============================] - 707s 381ms/step - loss: 1.4059 - accuracy: 0.3042 - val_loss: 1.4537 - val_accuracy: 0.3042\n",
      "Epoch 5/6\n",
      "1856/1856 [==============================] - ETA: 0s - loss: 1.3452 - accuracy: 0.3114\n",
      "Epoch 5: val_loss improved from 1.45368 to 1.43028, saving model to model_ruen\n",
      "1856/1856 [==============================] - 707s 381ms/step - loss: 1.3452 - accuracy: 0.3114 - val_loss: 1.4303 - val_accuracy: 0.3079\n",
      "Epoch 6/6\n",
      "1856/1856 [==============================] - ETA: 0s - loss: 1.3016 - accuracy: 0.3167\n",
      "Epoch 6: val_loss improved from 1.43028 to 1.42036, saving model to model_ruen\n",
      "1856/1856 [==============================] - 707s 381ms/step - loss: 1.3016 - accuracy: 0.3167 - val_loss: 1.4204 - val_accuracy: 0.3100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92735b1c90>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
    "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
    "             batch_size=512,\n",
    "             epochs=6,\n",
    "             callbacks=[checkpoint]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDmFPXiH2xRQ",
    "outputId": "6f9a9167-386a-4138-bc20-84cac64ee114"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_2_layer_call_fn while saving (showing 5 of 48). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/MyDrive/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: drive/MyDrive/model/assets\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f926d886810> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f926d450410> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f926ceb1050> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f926baef650> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f928b60f790> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f928f1d7490> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save('drive/MyDrive/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2whHuKHmwi80",
    "outputId": "afa6b16f-d986-469e-edab-8c3cc2d58431"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9283fe4090>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('drive/MyDrive/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "foPIwnt54gFw"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return list(iter(lambda: tuple(islice(it, size)), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "hFZZi5mu3MBN"
   },
   "outputs": [],
   "source": [
    "def translate(texts, batch_size=128):\n",
    "  res = []\n",
    "  batches = chunk(texts, batch_size)\n",
    "  sep = tokenizer_ru.token_to_id('[SEP]')\n",
    "  \n",
    "  for batch in tqdm(batches):\n",
    "    res_sent = []\n",
    "\n",
    "    input_ids = [encode(text.lower(), tokenizer_en) for text in batch]\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids,\n",
    "                                                              maxlen=max_len_en,\n",
    "                                                              padding='post')\n",
    "    output_ids = [[tokenizer_ru.token_to_id('[CLS]')] for _ in range(len(input_ids))]\n",
    "\n",
    "    preds = model.predict((input_ids, tf.cast(output_ids, tf.int32)),\n",
    "                          batch_size=batch_size).argmax(2)[:, -1]\n",
    "    \n",
    "    for i in range(max_len_ru - 1):\n",
    "      output_ids = np.column_stack([output_ids, preds])\n",
    "      if i == max_len_ru - 2:\n",
    "        for output in output_ids:\n",
    "          if len(np.where(output==sep)[0]) == 0:\n",
    "            last_ind = -1\n",
    "          else:\n",
    "            last_ind = np.where(output==sep)[0][0]\n",
    "          res_sent.append(tokenizer_ru.decode(output[1:last_ind]))\n",
    "      else:\n",
    "        preds = model.predict((input_ids, tf.cast(output_ids, tf.int32)),\n",
    "                            batch_size=32).argmax(2)[:, -1]\n",
    "\n",
    "    res.extend(res_sent)\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "EMQK_w0v-Jq9"
   },
   "outputs": [],
   "source": [
    "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
    "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3JPn3I19dkO",
    "outputId": "6e3bbf67-4883-430d-ee9d-28d9fe4e9986"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:49<00:00,  6.83s/it]\n"
     ]
    }
   ],
   "source": [
    "translations = translate(en_sents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "hV47gvvU1gBd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "bleus = []\n",
    "\n",
    "for i, sentence in enumerate(translations):\n",
    "  reference = tokenizer_ru.encode(sentence).tokens\n",
    "  hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
    "\n",
    "bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UVPDqb7A4Vp",
    "outputId": "94bf7ca0-e6ce-4f83-ce84-ebbecae6d209"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.806687382400554"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(bleus)/len(bleus))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p29_AvjUmB5w"
   },
   "source": [
    "## Задание 2 (2 балла).\n",
    "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
    "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yU87DGHmCPg"
   },
   "source": [
    "Back translation - метод, при котором создаются дополнительные синтетические данные, он достаточно легок в применении, т.к. не требует модификации алгоритмов обучения MT. Такой метод позволяет получить дополнительные данные тогда, когда имеется большой корпус одного языка, а параллельный корпус при этом маленький. Алгоритм back translation  такой: исходные моноязычные данные переводятся на целевой язык, целевые моноязычные данные переводятся обратно, а затем эти синтетические данные добавляются к настоящим двуязычным данным. \n",
    "Для пары en-ru:\n",
    "- На нашем параллельном корпусе обучаем переводить модель с русского на английский \n",
    "- Генерируем предложения на английском языке по предложениям на русском языке с помощью различных параметров (например, Monte Carlo search)\n",
    "- Полученнные сгенерированные предложения добавляем в наш параллельный корпус \n",
    "- На полученном датасете обучаем МТ-модель на перевод с английского на русский язык"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HW11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
